beeline> !connect jdbc:hive2://192.168.1.212:10000

create table intelligentmaintenancesystem(
              number int, value String
               )
partitioned by (machine_singnal String, filename string)
row format delimited fields terminated by '\t';

create table intelligentmaintenancesystemORC(
              number int, value String
               )
partitioned by (machine_singnal String, filename string)
row format delimited fields terminated by '\t'
stored as orc
tblproperties ("orc.compress"="SNAPPY");


insert into  intelligentmaintenancesystemORC partition(machine_singnal='MT2_ae_rms',filename ='MT2_ae_dataset_rms') select number,value from intelligentmaintenancesystem where machine_singnal='MT2_ae_rms' and filename ='MT2_ae_dataset_rms';

sudo su hdfs -l -c 'hdfs dfsadmin -safemode leave'
truncate 'intelligentmaintenancesystem'
scan 'intelligentmaintenancesystem'
count 'intelligentmaintenancesystem'
hbase thrift start

create 'IMSPre-split','normal','inner','outer',SPLITS => ['0500000','1000000','1500000','2000000','2500000','3000000','3500000','4000000','4500000','5000000','5500000','6000000','6500000','7000000','7500000','8000000','8500000','9000000','9500000']
disable 'IMSPre-split'
drop 'IMSPre-split'
truncate 'IMSPre-split'
count 'IMSPre-split'

./kafka-topics.sh --zookeeper 192.168.1.211:2181 --create --topic IMS --partitions 8 --replication-factor 2
./kafka-topics.sh --zookeeper 192.168.1.212:2181 --list
./kafka-topics.sh --zookeeper 192.168.1.212:2181 --describe --topic  IMS
./kafka-console-producer.sh  --broker-list 192.168.1.213:6667 --topic IMS
./kafka-console-consumer.sh  --bootstrap-server 192.168.1.212:6667 -from-beginning --topic IMS --from-beginning
./kafka-topics.sh --create --zookeeper localhost:2181 --replication-factor 1 --partitions 1 --topic IMS

./spark-submit --master yarn --executor-cores 2  --num-executors 3  --executor-memory 1g  --driver-memory 1g --deploy-mode client  --executor-cores 2  --conf spark.serializer="org.apache.spark.serializer.KryoSerializer"       --conf spark.streaming.kafka.maxRatePerPartition="7000"   --conf spark.streaming.backpressure.enabled="true"   --class  com.foxconn.realTime.kafakToHbase  tmp/intelligentmaintenancesystem-1.0-SNAPSHOT-jar-with-dependencies.jar
java -classpath intelligentmaintenancesystem-1.0-SNAPSHOT-jar-with-dependencies.jar com.foxconn.test.argprint
nohup ./changeconfigexecute.sh ../jar/intelligentmaintenancesystem-1.0-SNAPSHOT-jar-with-dependencies.jar ../jar/customConfig1.properties  &
/usr/hdp/2.6.3.0-235/spark2/upload-zx
