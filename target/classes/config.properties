#注意不要出现参数值后面不要出现空格，以免出现意想不到的的问题
# jdbc配置

#jdbc.driver.class=com.mysql.jdbc.Driver
#jdbc.url=jdbc:mysql://linux1:3306/sparkmall181111?useUnicode=true&characterEncoding=utf8&rewriteBatchedStatements=true
#jdbc.user=root
#jdbc.password=000000


jdbc.datasource.size=10
jdbc.driver.class = com.mysql.cj.jdbc.Driver
jdbc.url = jdbc:mysql://localhost:3306/nba?useSSL=false&serverTimezone=Asia/Shanghai
jdbc.user = root
jdbc.password = 123456


# Kafka配置
kafka.broker.list=node2:6667,node3:6667,node4:6667

# Redis配置
redis.host=192.168.1.215
redis.port=6379

# hive 的数据库名（选配）
hive.database= test_1114
# 存放原始数据的路径在192.168.1.207中，根路径应该改为 /opt
data.root.directory = I:


# 用于 TcpRecv.scala  TcpSend.scala   的对应Kafka参数
# 开启接收Tcp消息的接收端host与port(发送端port与此一致)
tcp.send.host = localhost
tcp.send.port = 9977
#模拟信号文件所在目录
tcp.send.directory = G:\\input
# 两次tcp send的间隔时间。即两个信号直接的间隔
tcp.send.interval.time = 1000
# 收到数据写入kafka的主题
tcp.recv.tokafka.topic = IMS
# 写入hbase中时这个参数会变成行健rownumber的一部分，现时使用的是文件名
tcp.recv.tokafka.sign.name = 20190419141936_still_without_tool
# 写入hbase时的列簇。现时用信号状态
tcp.recv.tokafka.sign.status = inner
# 写入hbase时的列。现时用machine_singnal_frequency一起构成列名
tcp.recv.tokafka.sign.machine_singnal = MT1_x_feed
tcp.recv.tokafka.sign.frequency = 25600

# 用于消费kafkaToHbase
consumer.group.id = IMS10
